{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143c3270",
   "metadata": {},
   "source": [
    "# Build an Interactive Question Answering App with pgvector on Aurora PostgreSQL\n",
    "_**Using a pretrained LLM and PostgreSQL extension `pgvector` for similarity search on product documentation**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [SageMaker Model Hosting](#SageMaker-Model-Hosting)\n",
    "1. [Load data into Amazon Aurora PostgreSQL](#Open-source-extension-pgvector-for-PostgreSQL)\n",
    "1. [Evaluate Aurora PostgreSQL vector search results](#Evaluate-Aurora-PostgreSQL-vector-search-results)\n",
    "1. [Cleanup](#Cleanup)\n",
    "\n",
    "## Background\n",
    "\n",
    "Our use case focuses on answering questions over specific documents, relying solely on the information within those documents to generate accurate and context-aware answers. By combining the prowess of semantic search with the impressive capabilities of LLMs like sentence transformers, we will demonstrate how to build a Document Q & A system that leverages cutting-edge AI technologies.\n",
    "\n",
    "One of the core components of searching textually similar items is a fixed length sentence/word embedding i.e. a  “feature vector” that corresponds to that text. The reference word/sentence embedding typically are generated offline and must be stored so they can be efficiently searched. In this use case we are using a pretrained SentenceTransformer model `all-mpnet-base-v2` from [HuggingFace Transformers](https://huggingface.co/sentence-transformers/all-mpnet-base-v2).\n",
    "\n",
    "To enable efficient searches for textually similar items, we'll use Amazon SageMaker to generate fixed length sentence embeddings i.e “feature vectors” and use the Nearest Neighbor search in Amazon Aurora PostgreSQL using the extension `pgvector`. The PostgreSQL `pgvector` extension lets you store and search for points in vector space and find the \"nearest neighbors\" for those points. Use cases include text generation, search, chatbots, personalized recommendations (for example, items on the \"You may also like...\" on the Amazon shopping website), and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items: After some initial setup, we'll host the pretrained language model in SageMaker. Then generate feature vectors for Aurora User Guide as a PDF. Those feature vectors will be stored in Amazon Aurora PostgreSQL within a vector datatype. Next, we'll explore some sample text queries, and explore the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbb165",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required python libraries for the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U psycopg2-binary pgvector tqdm boto3 requests gensim fitz PyMuPDF==1.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2ba0b",
   "metadata": {},
   "source": [
    "### Enter document URL (as a PDF)\n",
    "\n",
    "We've used the [Aurora User Guide](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html) as an example. Replace this with any PDF available on your enterprise’s knowledge corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75189d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. 2 mins\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "pages_text = []\n",
    "\n",
    "doc_source = \"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ug.pdf\"\n",
    "\n",
    "with BytesIO( requests.get(doc_source).content ) as data:\n",
    "    with fitz.open(stream=data, filetype=\"pdf\") as doc:\n",
    "        for page in doc:\n",
    "            t = page.get_text().replace('\\n', ' ').replace('\\\\n', ' ').replace('  ', ' ')\n",
    "            t = re.sub(r\"(\\(p. [0-9]+\\)|[0-9]+\\s*$)\", \"\", t)\n",
    "            pages_text.append( t )\n",
    "\n",
    "del pages_text[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. < 1 mins\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "n_tokens = [ len(list(tokenize(x))) for x in pages_text ]\n",
    "max_tokens = 150\n",
    "pages_text_final = []\n",
    "tokens_final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. < 1 mins\n",
    "\n",
    "def split_text(text, mtoken):\n",
    "    text_a = text.split('. ')\n",
    "    token_a = [ len(list(tokenize(x))) for x in text_a ]\n",
    "    tcnt = 0\n",
    "    text = \"\"\n",
    "    for x, t in zip(token_a, text_a):\n",
    "        tcnt+=x\n",
    "        if tcnt < mtoken:\n",
    "            text = text + \". \" + t\n",
    "        if tcnt >= mtoken:\n",
    "            pages_text_final.append(text)\n",
    "            tokens_final.append( len(list(tokenize(text))) )\n",
    "            text = t\n",
    "            tcnt = x\n",
    "    pages_text_final.append(text)\n",
    "    tokens_final.append( len(list(tokenize(text))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. < 1 mins\n",
    "\n",
    "for token, text in zip(n_tokens, pages_text):\n",
    "    if token > max_tokens:\n",
    "        split_text(text, max_tokens)\n",
    "    else:\n",
    "        tokens_final.append(token)\n",
    "        pages_text_final.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. < 1 mins\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "documents = {}\n",
    "\n",
    "workers = 1 * cpu_count()\n",
    "\n",
    "chunksize = 32\n",
    "\n",
    "def generate_tokens(data):\n",
    "    r = {}\n",
    "    r['description'] = data\n",
    "    r['description_tokenized'] = '. '.join( [ ' '.join(list(tokenize(x))) for x in data.split('. ') ]  )\n",
    "    return r\n",
    "\n",
    "#Generate Embeddings\n",
    "document_list = process_map(generate_tokens, pages_text_final, max_workers=workers, chunksize=chunksize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d69c65",
   "metadata": {},
   "source": [
    "## SageMaker Model Hosting\n",
    "\n",
    "In this section will deploy the pretrained [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) Hugging Face SentenceTransformer model into SageMaker and generate 768 dimensional vector embeddings for our Aurora product documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b60cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. 1 mins\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. 4 mins\n",
    "\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "  'HF_MODEL_ID': 'sentence-transformers/all-mpnet-base-v2',\n",
    "  'HF_TASK': 'feature-extraction'\n",
    "}\n",
    "\n",
    "# Deploy Hugging Face Model \n",
    "predictor = HuggingFaceModel(\n",
    "               env=hub, # configuration for loading model from Hub\n",
    "               role=role, # iam role with permissions to create an Endpoint\n",
    "               transformers_version='4.26',\n",
    "               pytorch_version='1.13',\n",
    "               py_version='py39',\n",
    "            ).deploy(\n",
    "               initial_instance_count=1,\n",
    "               instance_type=\"ml.m5.xlarge\",\n",
    "               endpoint_name=\"apg-pg-vector\"\n",
    "            )\n",
    "print(f\"Hugging Face Model has been deployed successfully to SageMaker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14940048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: Approx. 1 mins\n",
    "    \n",
    "# https://github.com/UKPLab/sentence-transformers/issues/1107\n",
    "def cls_pooling(model_output):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    return [sublist[0] for sublist in model_output][0]\n",
    "\n",
    "data = { \"inputs\": document_list[9].get('description_tokenized') }\n",
    "\n",
    "result = cls_pooling( predictor.predict(data=data) )\n",
    "print (len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f00a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a job using realtime inference to generate embeddings ~40 min.\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    r = {}\n",
    "    r['description'] = data.get('description')\n",
    "    inp = {'inputs' : data.get('description_tokenized') }\n",
    "    try:\n",
    "        r['descriptions_embeddings'] = cls_pooling( predictor.predict(data = inp) )\n",
    "    except:\n",
    "        r['descriptions_embeddings'] = None\n",
    "    return r\n",
    "\n",
    "workers = 1 * cpu_count()\n",
    "\n",
    "chunksize = 128\n",
    "\n",
    "#Generate Embeddings\n",
    "data_embeddings = process_map(generate_embeddings, document_list, max_workers=workers, chunksize=chunksize)\n",
    "\n",
    "document_list = data_embeddings.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbedd6",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector for PostgreSQL\n",
    "\n",
    "pgvector is an open-source extension for PostgreSQL that allows you to store and search vector embeddings for exact and approximate nearest neighbors. It is designed to work seamlessly with other PostgreSQL features, including indexing and querying.\n",
    "\n",
    "One of the key benefits of using pgvector is that it allows you to perform similarity searches on large datasets quickly and efficiently. One of the most common applications of generative AI and large language models (LLMs) in an enterprise environment is answering questions based on the enterprise’s knowledge corpus. Pre-trained foundation models (FMs) perform well at natural language understanding (NLU) tasks such summarization, text generation and question answering on a broad variety of topics. pgvector supports exact and approximate nearest neighbor search, L2 distance, inner product, and cosine distance.\n",
    "\n",
    "To further optimize your searches, you can also use pgvector's indexing features. By creating indexes on your vector data, you can speed up your searches and reduce the amount of time it takes to find the nearest neighbors to a given vector.\n",
    "\n",
    "In this step we'll get all the document text from the Aurora User Guide and store those embeddings into a  vector datatype in Amazon Aurora PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 2 minutes\n",
    "\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import boto3 \n",
    "import json \n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "response = client.get_secret_value(\n",
    "    SecretId='apgpg-vector-secret'\n",
    ")\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "\n",
    "cur = dbconn.cursor()\n",
    "cur.execute(\"set maintenance_work_mem='2GB';\")\n",
    "cur.execute(\"set work_mem='1GB';\")\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "register_vector(dbconn)\n",
    "cur.execute(\"DROP TABLE IF EXISTS documentation;\")\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS documentation(\n",
    "                  docsource text, \n",
    "                  n_tokens bigint, \n",
    "                  doctext text, \n",
    "                  embeddings vector(768) \n",
    "                  );\"\"\")\n",
    "\n",
    "for x in document_list:\n",
    "    n_tokens = len(list(tokenize( x.get('description') )))\n",
    "    cur.execute(\"\"\"INSERT INTO documentation\n",
    "                      (docsource, n_tokens, doctext, embeddings) \n",
    "                  VALUES(%s, %s, %s, %s);\"\"\", \n",
    "                  (doc_source, n_tokens, x.get('description'), x.get('descriptions_embeddings') ))\n",
    "\n",
    "cur.execute(\"\"\"CREATE INDEX ON documentation \n",
    "               USING ivfflat (embeddings vector_l2_ops) WITH (lists = 500);\"\"\")\n",
    "cur.execute(\"VACUUM ANALYZE documentation;\")\n",
    "\n",
    "cur.close()\n",
    "dbconn.close()\n",
    "print (\"Vector embeddings has been successfully loaded into PostgreSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3156551",
   "metadata": {},
   "source": [
    "## Evaluate Aurora PostgreSQL vector search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {\"inputs\": \"Aurora Global Database Write Forwarding\"}\n",
    "\n",
    "res1 = cls_pooling( predictor.predict(data=data) )\n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "response = client.get_secret_value(\n",
    "    SecretId='rdspg-vector-secret'\n",
    ")\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "cur = dbconn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"SELECT docsource, doctext\n",
    "  FROM (\n",
    "    SELECT docsource, doctext, n_tokens, embeddings,\n",
    "            (embeddings <=> %s) AS distances,\n",
    "            SUM(n_tokens) OVER (ORDER BY (embeddings <=> %s)) AS cum_n_tokens\n",
    "    FROM documentation\n",
    "    ) subquery\n",
    "  WHERE cum_n_tokens <= %s\n",
    "  ORDER BY distances ASC;\"\"\", \n",
    "            (np.array(res1),np.array(res1),600,))\n",
    "\n",
    "r = cur.fetchall()\n",
    "\n",
    "for x in r:\n",
    "    print (x[1])\n",
    "\n",
    "cur.close()\n",
    "dbconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4622a3a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58694e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
